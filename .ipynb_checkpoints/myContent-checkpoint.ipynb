{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e162353-342b-4757-8a5b-a19043123c51",
   "metadata": {},
   "source": [
    "# My Content — MVP Recommandation\n",
    "\n",
    "## Contexte\n",
    "My Content veut encourager la lecture en recommandant des contenus pertinents.  \n",
    "Nous développons un premier MVP basé sur un dataset public de logs de navigation (clics), sessions utilisateurs, métadonnées et embeddings d’articles (dataset Globo/G1 utilisé dans les travaux CHAMELEON).\n",
    "\n",
    "## Objectif MVP\n",
    "**En tant qu’utilisateur, je reçois une sélection de cinq articles.**\n",
    "\n",
    "Le MVP doit démontrer :\n",
    "- qu’on peut recommander **sans données internes** (avec le dataset public),\n",
    "- qu’on peut gérer le **cold start** (nouveaux utilisateurs) et l’arrivée de **nouveaux articles**,\n",
    "- qu’on peut **déployer** la solution (Azure Function + interface simple).\n",
    "\n",
    "## Approche du notebook\n",
    "Nous commençons par une baseline **transparente et métier**, facile à expliquer et à déployer :\n",
    "- **Trending** : recommander les articles les plus consultés (fallback cold start user),\n",
    "- **Item-Item co-clic** : recommander des articles “souvent lus ensemble” dans les sessions (personnalisation légère).\n",
    "\n",
    "Ensuite, nous mettrons en place un protocole d’évaluation **top-K** (ex. HR@5, MRR@5) adapté aux données implicites, puis nous discuterons l’extension vers une approche hybride exploitant davantage les embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb78f22-45e2-4099-8804-81bcb91fc76f",
   "metadata": {},
   "source": [
    "## Imports & configuration\n",
    "\n",
    "Cette cellule prépare l’environnement de travail :\n",
    "- import des bibliothèques (pandas/numpy),\n",
    "- définition des chemins vers les fichiers du dataset (clics, métadonnées, embeddings),\n",
    "- définition des constantes du MVP (TOP_K, seed),\n",
    "- standardisation des noms de colonnes attendues (user, session, item, timestamp).\n",
    "\n",
    "Objectif : avoir une base stable et lisible avant de charger les données et construire la baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38d9b899-9ce6-4668-93a4-83b48803b56e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config OK\n",
      "Clicks dir: data/clicks\n",
      "Meta path: data/articles_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Imports + config (baseline transparente)\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Chemins (à adapter)\n",
    "DATA_DIR = \"data\"  # ex: dossier où tu as dézippé clicks.zip + metadata + embeddings\n",
    "CLICKS_DIR = os.path.join(DATA_DIR, \"clicks\")  # dossier avec les CSV par heure\n",
    "ARTICLES_META_PATH = os.path.join(DATA_DIR, \"articles_metadata.csv\")\n",
    "EMBEDDINGS_PATH = os.path.join(DATA_DIR, \"articles_embeddings.pickle\")  # optionnel pour plus tard\n",
    "\n",
    "# Paramètres MVP\n",
    "TOP_K = 5\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Colonnes principales (dataset CHAMELEON)\n",
    "COL_USER = \"user_id\"\n",
    "COL_SESSION = \"session_id\"\n",
    "COL_ITEM = \"click_article_id\"\n",
    "COL_TS = \"click_timestamp\"  # timestamp du clic (ms)\n",
    "\n",
    "print(\"Config OK\")\n",
    "print(\"Clicks dir:\", CLICKS_DIR)\n",
    "print(\"Meta path:\", ARTICLES_META_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ff5098-a4bc-4b0f-91b1-8f285d584cf2",
   "metadata": {},
   "source": [
    "## Lister les fichiers de clics et estimer le volume\n",
    "\n",
    "Objectifs :\n",
    "- lister tous les fichiers `clicks` (CSV par heure),\n",
    "- estimer le volume total (nombre de fichiers, taille disque),\n",
    "- afficher un aperçu (plus gros fichiers, taille moyenne).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d0c9f75-1427-4f98-b93c-afcd94391a0e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de fichiers: 1\n",
      "Taille totale: 0.00 GB\n",
      "Taille moyenne: 0.13 MB | médiane: 0.13 MB | min: 0.13 MB | max: 0.13 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>size_mb</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clicks_sample.csv</td>\n",
       "      <td>0.127996</td>\n",
       "      <td>data/clicks/clicks_sample.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                file   size_mb                           path\n",
       "0  clicks_sample.csv  0.127996  data/clicks/clicks_sample.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Cellule — Listing + estimation volume (taille disque)\n",
    "\n",
    "click_files = sorted(glob.glob(os.path.join(CLICKS_DIR, \"*.csv\")))\n",
    "if len(click_files) == 0:\n",
    "    raise FileNotFoundError(f\"Aucun fichier .csv trouvé dans {CLICKS_DIR}\")\n",
    "\n",
    "sizes_bytes = np.array([os.path.getsize(p) for p in click_files], dtype=np.int64)\n",
    "\n",
    "total_gb = sizes_bytes.sum() / (1024**3)\n",
    "mean_mb = sizes_bytes.mean() / (1024**2)\n",
    "median_mb = np.median(sizes_bytes) / (1024**2)\n",
    "min_mb = sizes_bytes.min() / (1024**2)\n",
    "max_mb = sizes_bytes.max() / (1024**2)\n",
    "\n",
    "print(f\"Nombre de fichiers: {len(click_files)}\")\n",
    "print(f\"Taille totale: {total_gb:.2f} GB\")\n",
    "print(f\"Taille moyenne: {mean_mb:.2f} MB | médiane: {median_mb:.2f} MB | min: {min_mb:.2f} MB | max: {max_mb:.2f} MB\")\n",
    "\n",
    "# Affiche les 10 plus gros fichiers\n",
    "top_n = 10\n",
    "idx = np.argsort(-sizes_bytes)[:top_n]\n",
    "top_df = pd.DataFrame({\n",
    "    \"file\": [os.path.basename(click_files[i]) for i in idx],\n",
    "    \"size_mb\": [sizes_bytes[i] / (1024**2) for i in idx],\n",
    "    \"path\": [click_files[i] for i in idx],\n",
    "})\n",
    "display(top_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e380ac9-de67-4c29-82d7-ad04f0050db2",
   "metadata": {},
   "source": [
    "## Chargement d’un échantillon de logs de clics (sanity check)\n",
    "\n",
    "Objectifs :\n",
    "- vérifier que les fichiers de clics sont bien accessibles (CSV par heure),\n",
    "- charger un petit échantillon pour inspecter les colonnes et types,\n",
    "- valider la présence des colonnes clés (user, session, article, timestamp),\n",
    "- avoir un premier aperçu de la période (min/max timestamp) et du volume.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34b87454-a3b6-4416-a095-9208aaccba1c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de fichiers de clics trouvés: 1\n",
      "Exemples: ['data/clicks/clicks_sample.csv']\n",
      "\n",
      "Fichier échantillon: clicks_sample.csv\n",
      "Shape: (1883, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>session_start</th>\n",
       "      <th>session_size</th>\n",
       "      <th>click_article_id</th>\n",
       "      <th>click_timestamp</th>\n",
       "      <th>click_environment</th>\n",
       "      <th>click_deviceGroup</th>\n",
       "      <th>click_os</th>\n",
       "      <th>click_country</th>\n",
       "      <th>click_region</th>\n",
       "      <th>click_referrer_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1506825423271737</td>\n",
       "      <td>1506825423000</td>\n",
       "      <td>2</td>\n",
       "      <td>157541</td>\n",
       "      <td>1506826828020</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1506825423271737</td>\n",
       "      <td>1506825423000</td>\n",
       "      <td>2</td>\n",
       "      <td>68866</td>\n",
       "      <td>1506826858020</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1506825426267738</td>\n",
       "      <td>1506825426000</td>\n",
       "      <td>2</td>\n",
       "      <td>235840</td>\n",
       "      <td>1506827017951</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1506825426267738</td>\n",
       "      <td>1506825426000</td>\n",
       "      <td>2</td>\n",
       "      <td>96663</td>\n",
       "      <td>1506827047951</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1506825435299739</td>\n",
       "      <td>1506825435000</td>\n",
       "      <td>2</td>\n",
       "      <td>119592</td>\n",
       "      <td>1506827090575</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id        session_id  session_start  session_size  click_article_id  \\\n",
       "0        0  1506825423271737  1506825423000             2            157541   \n",
       "1        0  1506825423271737  1506825423000             2             68866   \n",
       "2        1  1506825426267738  1506825426000             2            235840   \n",
       "3        1  1506825426267738  1506825426000             2             96663   \n",
       "4        2  1506825435299739  1506825435000             2            119592   \n",
       "\n",
       "   click_timestamp  click_environment  click_deviceGroup  click_os  \\\n",
       "0    1506826828020                  4                  3        20   \n",
       "1    1506826858020                  4                  3        20   \n",
       "2    1506827017951                  4                  1        17   \n",
       "3    1506827047951                  4                  1        17   \n",
       "4    1506827090575                  4                  1        17   \n",
       "\n",
       "   click_country  click_region  click_referrer_type  \n",
       "0              1            20                    2  \n",
       "1              1            20                    2  \n",
       "2              1            16                    2  \n",
       "3              1            16                    2  \n",
       "4              1            24                    2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Colonnes: ['user_id', 'session_id', 'session_start', 'session_size', 'click_article_id', 'click_timestamp', 'click_environment', 'click_deviceGroup', 'click_os', 'click_country', 'click_region', 'click_referrer_type']\n",
      "\n",
      "Dtypes:\n",
      " user_id                int64\n",
      "session_id             int64\n",
      "session_start          int64\n",
      "session_size           int64\n",
      "click_article_id       int64\n",
      "click_timestamp        int64\n",
      "click_environment      int64\n",
      "click_deviceGroup      int64\n",
      "click_os               int64\n",
      "click_country          int64\n",
      "click_region           int64\n",
      "click_referrer_type    int64\n",
      "dtype: object\n",
      "\n",
      "Colonnes manquantes: set()\n",
      "\n",
      "Période (échantillon) :\n",
      " - min: 2017-10-01 03:00:00.026000\n",
      " - max: 2017-10-03 02:35:54.157000\n",
      " - % NaT: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Lister les fichiers + charger un échantillon\n",
    "\n",
    "click_files = sorted(glob.glob(os.path.join(CLICKS_DIR, \"*.csv\")))\n",
    "print(f\"Nombre de fichiers de clics trouvés: {len(click_files)}\")\n",
    "print(\"Exemples:\", click_files[:3])\n",
    "\n",
    "# Charge un seul fichier (petit) pour inspection\n",
    "sample_path = click_files[0]\n",
    "clicks_sample = pd.read_csv(sample_path)\n",
    "\n",
    "print(\"\\nFichier échantillon:\", os.path.basename(sample_path))\n",
    "print(\"Shape:\", clicks_sample.shape)\n",
    "display(clicks_sample.head())\n",
    "\n",
    "print(\"\\nColonnes:\", list(clicks_sample.columns))\n",
    "print(\"\\nDtypes:\\n\", clicks_sample.dtypes)\n",
    "\n",
    "# Vérifie que les colonnes attendues existent\n",
    "required_cols = {COL_USER, COL_SESSION, COL_ITEM, COL_TS}\n",
    "missing = required_cols - set(clicks_sample.columns)\n",
    "print(\"\\nColonnes manquantes:\", missing)\n",
    "\n",
    "# Aperçu rapide des timestamps (si colonne présente)\n",
    "if COL_TS in clicks_sample.columns:\n",
    "    ts = pd.to_datetime(clicks_sample[COL_TS], unit=\"ms\", errors=\"coerce\")\n",
    "    print(\"\\nPériode (échantillon) :\")\n",
    "    print(\" - min:\", ts.min())\n",
    "    print(\" - max:\", ts.max())\n",
    "    print(\" - % NaT:\", ts.isna().mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed06041a-0fe9-4969-85ed-3704389c885f",
   "metadata": {},
   "source": [
    "## Charger un sous-ensemble “train/test” (split temporel simple)\n",
    "\n",
    "Objectif :\n",
    "- Charger un bloc de fichiers horaires pour itérer vite (ex: N heures pour train, M heures pour test).\n",
    "- Construire un split réaliste basé sur le temps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb7c35db-4956-4c48-95f1-9a59cc764596",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (118339, 4)\n",
      "Test shape : (13216, 4)\n",
      "Train ts: 2017-10-01 03:00:00.026000 -> 2017-10-24 23:48:51.578000\n",
      "Test  ts: 2017-10-02 02:36:25.019000 -> 2017-10-11 05:33:25.108000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Charge une fenêtre temporelle (par nombre de fichiers)\n",
    "# Ajuste N_TRAIN_FILES / N_TEST_FILES selon la puissance de ta machine.\n",
    "\n",
    "N_TRAIN_FILES = 24\n",
    "N_TEST_FILES = 6\n",
    "\n",
    "click_files = sorted(glob.glob(os.path.join(CLICKS_DIR, \"*.csv\")))\n",
    "train_files = click_files[:N_TRAIN_FILES]\n",
    "test_files = click_files[N_TRAIN_FILES:N_TRAIN_FILES + N_TEST_FILES]\n",
    "\n",
    "def load_clicks(files: list[str]) -> pd.DataFrame:\n",
    "    # Charge une liste de CSV et garde uniquement les colonnes utiles à la baseline.\n",
    "    dfs = []\n",
    "    for p in files:\n",
    "        df = pd.read_csv(p)\n",
    "        dfs.append(df[[COL_USER, COL_SESSION, COL_ITEM, COL_TS]])\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "clicks_train = load_clicks(train_files)\n",
    "clicks_test = load_clicks(test_files)\n",
    "\n",
    "print(\"Train shape:\", clicks_train.shape)\n",
    "print(\"Test shape :\", clicks_test.shape)\n",
    "\n",
    "# Conversion timestamp en datetime pour tri et contrôles\n",
    "clicks_train[\"ts\"] = pd.to_datetime(clicks_train[COL_TS], unit=\"ms\", errors=\"coerce\")\n",
    "clicks_test[\"ts\"] = pd.to_datetime(clicks_test[COL_TS], unit=\"ms\", errors=\"coerce\")\n",
    "\n",
    "print(\"Train ts:\", clicks_train[\"ts\"].min(), \"->\", clicks_train[\"ts\"].max())\n",
    "print(\"Test  ts:\", clicks_test[\"ts\"].min(), \"->\", clicks_test[\"ts\"].max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8148552-cfd9-48a4-a244-1cf3df5e3817",
   "metadata": {},
   "source": [
    "## Préparation des données (nettoyage minimal)\n",
    "\n",
    "Objectifs :\n",
    "- supprimer les lignes invalides (ids manquants, timestamp invalide),\n",
    "- garantir les types (int),\n",
    "- trier les clics dans chaque session (ordre chronologique),\n",
    "- enlever les doublons (même article dans la même session).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15fb69eb-2233-4886-9636-be3c9892248c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train prepared: (118339, 5)\n",
      "Test prepared : (13216, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>session_id</th>\n",
       "      <th>click_article_id</th>\n",
       "      <th>click_timestamp</th>\n",
       "      <th>ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1506825423271737</td>\n",
       "      <td>157541</td>\n",
       "      <td>1506826828020</td>\n",
       "      <td>2017-10-01 03:00:28.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1506825423271737</td>\n",
       "      <td>68866</td>\n",
       "      <td>1506826858020</td>\n",
       "      <td>2017-10-01 03:00:58.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1506825426267738</td>\n",
       "      <td>235840</td>\n",
       "      <td>1506827017951</td>\n",
       "      <td>2017-10-01 03:03:37.951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1506825426267738</td>\n",
       "      <td>96663</td>\n",
       "      <td>1506827047951</td>\n",
       "      <td>2017-10-01 03:04:07.951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1506825435299739</td>\n",
       "      <td>119592</td>\n",
       "      <td>1506827090575</td>\n",
       "      <td>2017-10-01 03:04:50.575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id        session_id  click_article_id  click_timestamp  \\\n",
       "0        0  1506825423271737            157541    1506826828020   \n",
       "1        0  1506825423271737             68866    1506826858020   \n",
       "2        1  1506825426267738            235840    1506827017951   \n",
       "3        1  1506825426267738             96663    1506827047951   \n",
       "4        2  1506825435299739            119592    1506827090575   \n",
       "\n",
       "                       ts  \n",
       "0 2017-10-01 03:00:28.020  \n",
       "1 2017-10-01 03:00:58.020  \n",
       "2 2017-10-01 03:03:37.951  \n",
       "3 2017-10-01 03:04:07.951  \n",
       "4 2017-10-01 03:04:50.575  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Préparation des données, nettoyage\n",
    "\n",
    "def prepare_clicks(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Garde uniquement les lignes exploitables\n",
    "    x = df.dropna(subset=[COL_USER, COL_SESSION, COL_ITEM, \"ts\"]).copy()\n",
    "\n",
    "    # Types stables\n",
    "    x[COL_USER] = x[COL_USER].astype(int)\n",
    "    x[COL_SESSION] = x[COL_SESSION].astype(int)\n",
    "    x[COL_ITEM] = x[COL_ITEM].astype(int)\n",
    "\n",
    "    # Tri chronologique dans chaque session (utile pour next-click)\n",
    "    x = x.sort_values([COL_SESSION, \"ts\"], ascending=[True, True])\n",
    "\n",
    "    # Un article ne doit compter qu'une fois par session\n",
    "    x = x.drop_duplicates(subset=[COL_SESSION, COL_ITEM], keep=\"first\")\n",
    "    return x\n",
    "\n",
    "clicks_train_p = prepare_clicks(clicks_train)\n",
    "clicks_test_p = prepare_clicks(clicks_test)\n",
    "\n",
    "print(\"Train prepared:\", clicks_train_p.shape)\n",
    "print(\"Test prepared :\", clicks_test_p.shape)\n",
    "display(clicks_train_p.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3380a368-8151-458b-a474-e84a00157799",
   "metadata": {},
   "source": [
    "## Baseline : Trending (popularité)\n",
    "\n",
    "Objectifs :\n",
    "- calculer les articles les plus cliqués dans le train,\n",
    "- servir ces articles comme recommandation pour tout le monde (cold start).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f10728a0-3f4f-4794-9b05-34e475c399d1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>272660</td>\n",
       "      <td>7742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>207122</td>\n",
       "      <td>7347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>161178</td>\n",
       "      <td>6660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>284463</td>\n",
       "      <td>6497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>160474</td>\n",
       "      <td>5020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>59758</td>\n",
       "      <td>4565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>313504</td>\n",
       "      <td>4469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>119592</td>\n",
       "      <td>4410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>96663</td>\n",
       "      <td>4245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>235132</td>\n",
       "      <td>3215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id  score\n",
       "0      272660   7742\n",
       "1      207122   7347\n",
       "2      161178   6660\n",
       "3      284463   6497\n",
       "4      160474   5020\n",
       "5       59758   4565\n",
       "6      313504   4469\n",
       "7      119592   4410\n",
       "8       96663   4245\n",
       "9      235132   3215"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Trending pour la baseline\n",
    "\n",
    "def build_trending(df_train: pd.DataFrame, top_n: int = 2000) -> pd.DataFrame:\n",
    "    # Compte les clics par article et garde les plus populaires\n",
    "    vc = df_train[COL_ITEM].value_counts().head(top_n)\n",
    "    trending_df = vc.rename_axis(\"article_id\").reset_index(name=\"score\")\n",
    "    trending_df[\"article_id\"] = trending_df[\"article_id\"].astype(int)\n",
    "    return trending_df\n",
    "\n",
    "trending = build_trending(clicks_train_p, top_n=2000)\n",
    "display(trending.head(10))\n",
    "\n",
    "# Liste utilisée par le recommender\n",
    "TRENDING_LIST = trending[\"article_id\"].tolist()\n",
    "\n",
    "def recommend_trending(k: int = TOP_K) -> list[int]:\n",
    "    # Retourne les K articles les plus populaires\n",
    "    return TRENDING_LIST[:k]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1b2466-0567-4831-9465-f9fcd924a144",
   "metadata": {},
   "source": [
    "## Baseline : Item-Item co-clic (voisins par article)\n",
    "\n",
    "Objectifs :\n",
    "- construire, à partir du train, les articles souvent vus dans la même session,\n",
    "- produire une table `article_id -> top voisins`,\n",
    "- proposer des recommandations personnalisées à partir des derniers amrticles vus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37250e31-33d3-4459-9160-650442818920",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>neighbor_id</th>\n",
       "      <th>rank</th>\n",
       "      <th>co_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>271</td>\n",
       "      <td>205420</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>290</td>\n",
       "      <td>331293</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>388</td>\n",
       "      <td>239459</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>683</td>\n",
       "      <td>313504</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1916</td>\n",
       "      <td>59758</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1916</td>\n",
       "      <td>64329</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1916</td>\n",
       "      <td>95524</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1916</td>\n",
       "      <td>95633</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1916</td>\n",
       "      <td>118751</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1916</td>\n",
       "      <td>156229</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id  neighbor_id  rank  co_count\n",
       "0         271       205420     1         1\n",
       "1         290       331293     1         1\n",
       "2         388       239459     1         1\n",
       "3         683       313504     1         1\n",
       "4        1916        59758     1         1\n",
       "5        1916        64329     2         1\n",
       "6        1916        95524     3         1\n",
       "7        1916        95633     4         1\n",
       "8        1916       118751     5         1\n",
       "9        1916       156229     6         1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Item-Item-Co-Click - Recommend from history\n",
    "\n",
    "# Construit une table item-item basée sur la co-occurrence en session:\n",
    "# - article_id = article source\n",
    "# - neighbor_id = article souvent vu avec l'article source\n",
    "# - co_count = nombre de sessions où ils co-apparaissent\n",
    "# - rank = position du voisin (1 = meilleur)\n",
    "\n",
    "def build_item_item_coclick(df_train: pd.DataFrame, k_neighbors: int = 100) -> pd.DataFrame:\n",
    "    # On ne garde que session + article\n",
    "    sess_items = df_train[[COL_SESSION, COL_ITEM]].copy()\n",
    "    sess_items = sess_items.rename(columns={COL_SESSION: \"session_id\", COL_ITEM: \"article_id\"})\n",
    "\n",
    "    # Sécurité: un article ne compte qu'une fois par session\n",
    "    sess_items = sess_items.drop_duplicates(subset=[\"session_id\", \"article_id\"], keep=\"first\")\n",
    "\n",
    "    # Auto-join par session pour obtenir toutes les paires co-cliquées (a,b)\n",
    "    left = sess_items.rename(columns={\"article_id\": \"a\"})\n",
    "    right = sess_items.rename(columns={\"article_id\": \"b\"})\n",
    "    pairs = left.merge(right, on=\"session_id\", how=\"inner\")\n",
    "\n",
    "    # On enlève les paires identiques (a == b)\n",
    "    pairs = pairs[pairs[\"a\"] != pairs[\"b\"]]\n",
    "\n",
    "    # Compte de co-occurrence: nombre de sessions contenant (a,b)\n",
    "    counts = (\n",
    "        pairs.groupby([\"a\", \"b\"], as_index=False)\n",
    "        .size()\n",
    "        .rename(columns={\"size\": \"co_count\"})\n",
    "    )\n",
    "\n",
    "    # Top voisins par article a\n",
    "    counts = counts.sort_values([\"a\", \"co_count\"], ascending=[True, False])\n",
    "    counts[\"rank\"] = counts.groupby(\"a\").cumcount() + 1\n",
    "    topk = counts[counts[\"rank\"] <= k_neighbors].copy()\n",
    "\n",
    "    # Format standard\n",
    "    item_item_df = topk.rename(columns={\"a\": \"article_id\", \"b\": \"neighbor_id\"})\n",
    "    return item_item_df[[\"article_id\", \"neighbor_id\", \"rank\", \"co_count\"]]\n",
    "\n",
    "# Construction de la table item-item\n",
    "item_item = build_item_item_coclick(clicks_train_p, k_neighbors=100)\n",
    "display(item_item.head(10))\n",
    "\n",
    "# Dictionnaire: article_id -> liste des voisins (par ordre de rank)\n",
    "neighbors_map = (\n",
    "    item_item.sort_values([\"article_id\", \"rank\"])\n",
    "    .groupby(\"article_id\")[\"neighbor_id\"]\n",
    "    .apply(list)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "def recommend_item_item_from_history(history: list[int], k: int = TOP_K) -> list[int]:\n",
    "    # Agrège les voisins des derniers articles vus.\n",
    "    # Score simple: somme(1/rank) pour les voisins proposés.\n",
    "    scores = {}\n",
    "    seen = set(history)\n",
    "\n",
    "    for item in history[-5:]:\n",
    "        for rank, neigh in enumerate(neighbors_map.get(item, []), start=1):\n",
    "            if neigh in seen:\n",
    "                continue\n",
    "            scores[neigh] = scores.get(neigh, 0.0) + (1.0 / rank)\n",
    "\n",
    "    ranked = [aid for aid, _ in sorted(scores.items(), key=lambda t: t[1], reverse=True)]\n",
    "\n",
    "    # Complète avec trending si nécessaire\n",
    "    out = []\n",
    "    for aid in ranked:\n",
    "        if aid not in out:\n",
    "            out.append(aid)\n",
    "        if len(out) >= k:\n",
    "            return out\n",
    "\n",
    "    for aid in TRENDING_LIST:\n",
    "        if aid not in out and aid not in seen:\n",
    "            out.append(aid)\n",
    "        if len(out) >= k:\n",
    "            break\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbe71d0-aa7e-4d42-a343-571d056855ee",
   "metadata": {},
   "source": [
    "## Où on en est (baseline recommandation)\n",
    "\n",
    "À ce stade, on a construit deux stratégies simples à partir des clics du **train** :\n",
    "\n",
    "### 1) Trending (popularité)\n",
    "- On compte combien de fois chaque article a été cliqué dans le train.\n",
    "- On trie du plus cliqué au moins cliqué.\n",
    "- On recommande les **K** premiers à tout le monde.\n",
    "- Utilité : **fallback** quand on ne sait rien sur l’utilisateur (cold start user).\n",
    "\n",
    "### 2) Item-Item co-clic (articles “souvent vus ensemble”)\n",
    "**Item = article**, et **co-clic = co-occurrence dans une même session**.\n",
    "\n",
    "Exemple de sessions (paniers) :\n",
    "- S1 : [A, B, C]\n",
    "- S2 : [A, B]\n",
    "- S3 : [A, D]\n",
    "\n",
    "On en déduit que :\n",
    "- A est souvent associé à B → B est un bon candidat à recommander après A.\n",
    "\n",
    "### Ce que représente `item_item_coclick` vs `item_item`\n",
    "- `item_item_coclick` : la **méthode** (calculer des voisinages à partir des co-clics en session).\n",
    "- `item_item` : le **résultat** (une table / DataFrame) avec, pour chaque article, ses voisins.\n",
    "\n",
    "Chaque ligne de `item_item` contient typiquement :\n",
    "- `article_id` : l’article “source” (ex: A)\n",
    "- `neighbor_id` : un article “voisin” (ex: B)\n",
    "- `co_count` : nombre de sessions où A et B apparaissent ensemble\n",
    "- `rank` : position de B parmi les voisins de A (1 = meilleur voisin)\n",
    "\n",
    "Autrement dit :\n",
    "> `item_item` = “pour chaque article, les meilleurs articles associés selon les sessions”.\n",
    "\n",
    "### Comment on recommande pour un utilisateur\n",
    "1) On récupère l’historique récent de l’utilisateur (ex: [A, C]).\n",
    "2) On prend les voisins de A et les voisins de C (via `item_item`).\n",
    "3) On fusionne, on enlève les doublons et les articles déjà vus.\n",
    "4) On garde les **K** meilleurs candidats.\n",
    "5) S’il manque des articles, on complète avec **Trending**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcedbaeb-a519-43c0-9908-a289efc08483",
   "metadata": {},
   "source": [
    "## Évaluation simple (next-click) : HR@5 et MRR@5\n",
    "\n",
    "Objectif :\n",
    "- simuler une recommandation en cours de session,\n",
    "- comparer deux baselines : Trending vs Item-Item co-clic,\n",
    "- calculer :\n",
    "  - **HR@5** : le prochain clic est-il dans le top-5 ?\n",
    "  - **MRR@5** : s’il y est, est-il plutôt en haut de la liste ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9146ffa8-2fd7-4ea6-993e-12b1dce12198",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def hr_mrr_at_k(true_item: int, recs: list[int], k: int = TOP_K) -> tuple[float, float]:\n",
    "    # HR@k = 1 si vrai item dans top-k, sinon 0\n",
    "    # MRR@k = 1/rang si présent, sinon 0\n",
    "    topk = recs[:k]\n",
    "    if true_item in topk:\n",
    "        rank = topk.index(true_item) + 1\n",
    "        return 1.0, 1.0 / rank\n",
    "    return 0.0, 0.0\n",
    "\n",
    "\n",
    "def evaluate_next_click(\n",
    "    df_test: pd.DataFrame,\n",
    "    recommend_fn,\n",
    "    k: int = TOP_K,\n",
    "    max_sessions: int | None = 5000,\n",
    ") -> pd.DataFrame:\n",
    "    # On évalue sur des sessions de test : pour chaque session, on prend un préfixe et on prédit le prochain clic.\n",
    "    # On utilise ici \"dernier préfixe\" : on donne tous les clics sauf le dernier, et on essaie de deviner le dernier.\n",
    "\n",
    "    rows = []\n",
    "    sessions = df_test.groupby(COL_SESSION)\n",
    "\n",
    "    # Option : limiter le nombre de sessions pour aller vite\n",
    "    session_items = list(sessions)\n",
    "    if max_sessions is not None:\n",
    "        session_items = session_items[:max_sessions]\n",
    "\n",
    "    for session_id, g in session_items:\n",
    "        items = g[COL_ITEM].astype(int).tolist()\n",
    "\n",
    "        # Il faut au moins 2 clics pour avoir \"contexte -> next item\"\n",
    "        if len(items) < 2:\n",
    "            continue\n",
    "\n",
    "        history = items[:-1]\n",
    "        true_next = items[-1]\n",
    "\n",
    "        recs = recommend_fn(history, k)\n",
    "        hr, mrr = hr_mrr_at_k(true_next, recs, k=k)\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"session_id\": int(session_id),\n",
    "                \"history_len\": len(history),\n",
    "                \"true_next\": int(true_next),\n",
    "                \"HR@5\": hr,\n",
    "                \"MRR@5\": mrr,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# Wrappers pour unifier les signatures (history, k)\n",
    "def recommend_trending_from_history(history: list[int], k: int = TOP_K) -> list[int]:\n",
    "    return recommend_trending(k=k)\n",
    "\n",
    "def recommend_coclick_from_history(history: list[int], k: int = TOP_K) -> list[int]:\n",
    "    return recommend_item_item_from_history(history, k=k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa6688d-67dd-49ea-b241-d1e04326f607",
   "metadata": {},
   "source": [
    "## Pourquoi il y a 2 baselines ici (et ce que fait la cellule)\n",
    "\n",
    "À ce stade, on a bien **2 baselines**, et la cellule sert uniquement à **les comparer**.\n",
    "\n",
    "### Baseline 1 — Trending (popularité)\n",
    "- Même 5 articles pour tout le monde : les plus cliqués dans le train.\n",
    "- Sert de **plancher** : “au minimum, recommander les articles populaires, ça donne quoi ?”.\n",
    "\n",
    "### Baseline 2 — Item-Item co-clic (personnalisation simple)\n",
    "- À partir des derniers articles cliqués dans une session, on recommande des articles **souvent co-cliqués** avec eux.\n",
    "- Sert de baseline “un cran au-dessus”, tout en restant **très transparente**.\n",
    "\n",
    "### Ce que fait la cellule d’évaluation\n",
    "1) Elle lance `evaluate_next_click(...)` avec la baseline **Trending** → on obtient HR@5 et MRR@5 sur des sessions de test.\n",
    "2) Elle relance la même évaluation avec la baseline **co-clic**.\n",
    "3) Elle calcule les moyennes et affiche un **résumé** pour comparer les deux.\n",
    "\n",
    "### Pourquoi on le fait maintenant\n",
    "Cela permet de dire clairement dans le projet :\n",
    "- “La baseline popularité fait X”\n",
    "- “La baseline co-clic fait Y”\n",
    "- “Donc notre approche apporte (ou non) un gain”\n",
    "\n",
    "Et ça aide à décider si ça vaut le coup de passer ensuite à une approche plus avancée (embeddings/LightFM).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4aeb87-7a54-48f7-b80f-2659bcf88b22",
   "metadata": {},
   "source": [
    "### Clarification\n",
    "\n",
    "- Réalité (ground truth) : l’article réellement cliqué ensuite (dans le test).\n",
    "- Trending : prédiction naïve basée sur la popularité globale.\n",
    "- Co-clic : prédiction plus personnalisée basée sur les associations d’articles en session.\n",
    "\n",
    "On compare les prédictions de chaque baseline à la réalité, avec HR@5 / MRR@5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb96eb4-96a3-4658-a06b-a24df2325dd0",
   "metadata": {},
   "source": [
    "## Lancer l’évaluation : Trending vs Co-clic\n",
    "\n",
    "Objectif :\n",
    "- obtenir un score moyen HR@5 et MRR@5 sur un échantillon de sessions test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d70c6719-6452-4fdc-8eb7-f2474ce81bb0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>n_sessions</th>\n",
       "      <th>HR@5</th>\n",
       "      <th>MRR@5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Trending</td>\n",
       "      <td>4896</td>\n",
       "      <td>0.175449</td>\n",
       "      <td>0.114560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Item-Item co-clic</td>\n",
       "      <td>4896</td>\n",
       "      <td>0.399510</td>\n",
       "      <td>0.215203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model  n_sessions      HR@5     MRR@5\n",
       "0           Trending        4896  0.175449  0.114560\n",
       "1  Item-Item co-clic        4896  0.399510  0.215203"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Évaluation: Trending vs Item-Item co-clic\n",
    "# On compare les deux baselines sur un sous-ensemble de sessions de test.\n",
    "# Résultats: moyenne de HR@5 et MRR@5.\n",
    "\n",
    "# Évalue la baseline Trending\n",
    "eval_trending = evaluate_next_click(\n",
    "    clicks_test_p,\n",
    "    recommend_fn=recommend_trending_from_history,  # wrapper: ignore l'historique, renvoie top popularité\n",
    "    k=TOP_K,\n",
    "    max_sessions=5000,  # limite pour itérer vite\n",
    ")\n",
    "\n",
    "# Évalue la baseline co-clic\n",
    "eval_coclick = evaluate_next_click(\n",
    "    clicks_test_p,\n",
    "    recommend_fn=recommend_coclick_from_history,  # wrapper: utilise l'historique pour proposer des voisins\n",
    "    k=TOP_K,\n",
    "    max_sessions=5000,\n",
    ")\n",
    "\n",
    "# Résumé: scores moyens + nombre de sessions évaluées\n",
    "summary = pd.DataFrame(\n",
    "    {\n",
    "        \"model\": [\"Trending\", \"Item-Item co-clic\"],\n",
    "        \"n_sessions\": [len(eval_trending), len(eval_coclick)],\n",
    "        \"HR@5\": [eval_trending[\"HR@5\"].mean(), eval_coclick[\"HR@5\"].mean()],\n",
    "        \"MRR@5\": [eval_trending[\"MRR@5\"].mean(), eval_coclick[\"MRR@5\"].mean()],\n",
    "    }\n",
    ")\n",
    "\n",
    "display(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c531e0-4f6f-44b5-b262-922e3c8e43fa",
   "metadata": {},
   "source": [
    "## Comprendre HR@5 et MRR@5 (évaluation next-click)\n",
    "\n",
    "### HR@5 (Hit Rate @ 5)\n",
    "Pour chaque session, on “cache” le dernier clic (le vrai prochain article) et on recommande 5 articles à partir de l’historique.\n",
    "- **HR@5 = 1** si le vrai article est **dans les 5** recommandations, sinon 0.\n",
    "- La moyenne (ex: 0.3995) correspond au **% de sessions** où on a “touché juste” dans le top-5.\n",
    "\n",
    "### MRR@5 (Mean Reciprocal Rank @ 5)\n",
    "Même principe, mais on prend en compte **la position** du bon article dans le top-5 :\n",
    "- bon article en **1ère position** → score 1\n",
    "- en **2e** → 1/2 = 0.5\n",
    "- en **3e** → 1/3 ≈ 0.33\n",
    "- … absent du top-5 → 0  \n",
    "La moyenne résume “à quelle hauteur” on place le bon article quand on le trouve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a0cb26-fcd5-45cd-b160-2ebe9bb8c195",
   "metadata": {},
   "source": [
    "## Interprétation des résultats obtenus\n",
    "\n",
    "| Modèle | HR@5 | MRR@5 |\n",
    "|---|---:|---:|\n",
    "| Trending | 0.175 | 0.115 |\n",
    "| Item-Item co-clic | 0.400 | 0.215 |\n",
    "\n",
    "### Lecture simple\n",
    "- **Trending (0.175)** : pour **17.5%** des sessions, le vrai prochain clic est dans les 5 recommandations.\n",
    "- **Co-clic (0.400)** : pour **40%** des sessions, le vrai prochain clic est dans les 5 recommandations.\n",
    "\n",
    "### Gain\n",
    "- Gain absolu HR@5 ≈ **+22 points** (0.400 - 0.175).\n",
    "- Le co-clic fait environ **2.3× mieux** que Trending (0.400 / 0.175 ≈ 2.28).\n",
    "\n",
    "### Conclusion MVP\n",
    "- **Trending** fournit un plancher (popularité seule).\n",
    "- **Item-Item co-clic** améliore fortement la pertinence et place plus souvent le bon article plus haut (MRR@5 plus élevé).\n",
    "- Pour une baseline transparente, ces résultats sont **très corrects** et montrent que le signal session est bien exploité.\n",
    "\n",
    "### Nuances\n",
    "- Les scores dépendent du split (fenêtre temporelle, taille train/test).\n",
    "- Ici on a évalué une version simple “dernier clic de session”; on pourra raffiner plus tard (plusieurs positions dans la session).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ee7ed0-7b2f-407d-886a-cd1848e3164f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ef3f4d-7db5-49d7-9e3f-37233673ece7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28445c27-a3a2-41cb-b2e6-2ba9cd403ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8bde7681-13f1-49e9-afd9-0e3a9267973c",
   "metadata": {},
   "source": [
    "## Comparer un modèle (ex. Surprise) aux baselines\n",
    "\n",
    "Oui : quand on testera un modèle “plus avancé” (Surprise ou autre), on le comparera aux baselines.\n",
    "\n",
    "### Baseline popularité (Trending)\n",
    "- C’est le **plancher** incontournable.\n",
    "- Si le modèle ne fait pas mieux que Trending, il ne vaut pas le coût (complexité / déploiement).\n",
    "\n",
    "### Baseline co-clic (Item-Item)\n",
    "- Baseline “simple mais forte” en recommandation session-based.\n",
    "- Si le modèle ne dépasse pas co-clic, il est difficile de le justifier.\n",
    "\n",
    "### Important pour ce projet\n",
    "- Le dataset est **implicite + orienté sessions** (next-click).\n",
    "- Donc l’évaluation restera centrée sur des métriques top-K comme **HR@5** et **MRR@5**,\n",
    "  sur le même split (temporel ou par sessions) pour comparer équitablement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e96589d-30db-401f-a25d-8f21be8ce994",
   "metadata": {},
   "source": [
    "## Pourquoi Surprise peut être “hors-sujet méthodo” pour ce dataset\n",
    "\n",
    "### 1) Surprise est pensé pour des **notes explicites**\n",
    "Les algorithmes de Surprise (SVD, KNN, etc.) cherchent à prédire une **valeur de rating** (ex : 1 à 5).  \n",
    "Ici, on n’a pas de notes : on a des **clics** (feedback implicite) et des **sessions**.\n",
    "\n",
    "-> Pour utiliser Surprise, il faut **inventer** une “note” artificielle (ex : clic = 1).  \n",
    "Le problème : on ne sait pas quoi faire des **non-clics** (0 ? absent ?) et ces choix peuvent fortement biaiser le modèle.\n",
    "\n",
    "### 2) Le dataset est **session-based / next-click**\n",
    "L’objectif naturel ici est de prédire le **prochain clic** dans une session (séquence A → B → C).  \n",
    "Surprise modélise plutôt une préférence globale user–item et ne capture pas directement la **séquence** et la **récence**.\n",
    "\n",
    "### 3) Conséquence : perte de temps et risque de résultats moins pertinents\n",
    "On passe du temps à :\n",
    "- définir une pseudo-note,\n",
    "- justifier la transformation,\n",
    "- tout en n’exploitant pas le signal principal du dataset (sessions / next-click).\n",
    "\n",
    "-> C’est pour cela que Surprise peut être considéré “hors-sujet” méthodologiquement ici, même s’il est facile à installer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a63943d-6238-4763-a7ca-0af6494ff738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03bad85a-ed5f-42ce-9467-56d6272d627e",
   "metadata": {},
   "source": [
    "- **Trending (popularité)** = le **plancher incontournable** : si ton modèle (Surprise ou autre) ne fait pas mieux, il ne vaut pas le coût.\n",
    "\n",
    "- **Co-clic item-item** = une baseline **simple mais forte** en session-based : si ton modèle ne la dépasse pas, il est difficile de le justifier.\n",
    "\n",
    "- **Attention** : Surprise est surtout fait pour des **notes explicites**, alors que ton dataset est **implicite + session-based** ; tu peux le tester, mais l’évaluation restera la même idée : **HR@5 / MRR@5** comparés aux baselines sur le **même split**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d2bf84-fad4-41e3-bc13-d3a1c500e398",
   "metadata": {},
   "source": [
    "## LightFM — Préparation (imports + chargement des embeddings)\n",
    "\n",
    "Objectifs :\n",
    "- importer LightFM (modèle hybride implicite),\n",
    "- charger les embeddings d’articles déjà fournis (250 dimensions),\n",
    "- inspecter la structure du pickle (selon la source, ce peut être une matrice ou un dict).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1460445b-8de5-4d7a-b861-ef891dad410d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy.char'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/MyContent/lib/python3.11/site-packages/lightfm/__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43m__LIGHTFM_SETUP__\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name '__LIGHTFM_SETUP__' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Imports nécessaires au modèle (LightFM) et aux matrices creuses (sparse)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightfm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LightFM\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m csr_matrix\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpickle\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/MyContent/lib/python3.11/site-packages/lightfm/__init__.py:4\u001b[39m\n\u001b[32m      2\u001b[39m     __LIGHTFM_SETUP__\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlightfm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LightFM\n\u001b[32m      6\u001b[39m __version__ = \u001b[33m\"\u001b[39m\u001b[33m1.17\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mLightFM\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdatasets\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mevaluation\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/MyContent/lib/python3.11/site-packages/lightfm/lightfm.py:6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msp\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lightfm_fast\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     CSRMatrix,\n\u001b[32m     10\u001b[39m     FastLightFM,\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     predict_ranks,\n\u001b[32m     17\u001b[39m )\n\u001b[32m     19\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mLightFM\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/MyContent/lib/python3.11/site-packages/scipy/__init__.py:67\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _key.startswith(\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m _fun = \u001b[38;5;28mgetattr\u001b[39m(np, _key)\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_fun, _types.ModuleType):\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/MyContent/lib/python3.11/site-packages/numpy/__init__.py:748\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(attr)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy.char'"
     ]
    }
   ],
   "source": [
    "# Imports nécessaires au modèle (LightFM) et aux matrices creuses (sparse)\n",
    "from lightfm import LightFM\n",
    "from scipy.sparse import csr_matrix\n",
    "import pickle\n",
    "\n",
    "# Chargement des embeddings (pickle)\n",
    "# Selon le dataset, ce pickle peut contenir:\n",
    "# - soit une matrice numpy (N_articles x 250)\n",
    "# - soit un dict contenant une matrice + une liste d'ids\n",
    "with open(EMBEDDINGS_PATH, \"rb\") as f:\n",
    "    emb_obj = pickle.load(f)\n",
    "\n",
    "print(\"Type embeddings pickle:\", type(emb_obj))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ba9ff8-ef94-4676-9646-2ac30c295af9",
   "metadata": {},
   "source": [
    "## LightFM — Construire une table `article_id -> embedding`\n",
    "\n",
    "Objectifs :\n",
    "- construire un DataFrame `emb_df` indexé par `article_id`,\n",
    "- avoir les colonnes d’embeddings `e0..e249`,\n",
    "- aligner correctement les lignes d'embeddings avec les ids d’articles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23f2d92-9201-44ed-9264-2915e7609c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des métadonnées articles: permet d'identifier la colonne \"article_id\"\n",
    "articles_meta = pd.read_csv(ARTICLES_META_PATH)\n",
    "\n",
    "print(\"articles_metadata shape:\", articles_meta.shape)\n",
    "display(articles_meta.head(3))\n",
    "\n",
    "# Détection (simple) de la colonne id article\n",
    "possible_id_cols = [c for c in articles_meta.columns if \"article\" in c and \"id\" in c]\n",
    "print(\"Colonnes candidates pour article_id:\", possible_id_cols)\n",
    "\n",
    "# Choix de la colonne article_id (ajuste si nécessaire)\n",
    "ARTICLE_ID_COL = \"article_id\" if \"article_id\" in articles_meta.columns else possible_id_cols[0]\n",
    "print(\"ARTICLE_ID_COL retenue =\", ARTICLE_ID_COL)\n",
    "\n",
    "\n",
    "# Fonction: construire un DataFrame embeddings\n",
    "# - Si le pickle est un dict: on récupère ids + matrice\n",
    "# - Si le pickle est une matrice: on suppose qu'elle est dans le même ordre que articles_metadata\n",
    "def build_embeddings_df(emb_obj, articles_meta: pd.DataFrame, article_id_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Construit un DataFrame avec une ligne par article:\n",
    "    - colonne article_id\n",
    "    - colonnes e0..e(D-1) pour les embeddings\n",
    "    \"\"\"\n",
    "    # Cas 1: pickle = dict\n",
    "    if isinstance(emb_obj, dict):\n",
    "        # Cherche une clé d'ids\n",
    "        ids = None\n",
    "        for key_ids in [\"article_id\", \"article_ids\", \"ids\"]:\n",
    "            if key_ids in emb_obj:\n",
    "                ids = np.array(emb_obj[key_ids]).astype(int)\n",
    "                break\n",
    "\n",
    "        # Cherche une clé de matrice embeddings\n",
    "        mat = None\n",
    "        for key_mat in [\"embeddings\", \"matrix\", \"X\"]:\n",
    "            if key_mat in emb_obj:\n",
    "                mat = np.array(emb_obj[key_mat])\n",
    "                break\n",
    "\n",
    "        if ids is None or mat is None:\n",
    "            raise ValueError(\"Pickle dict non reconnu: attend des clés ids + matrice embeddings\")\n",
    "\n",
    "        df = pd.DataFrame(mat, columns=[f\"e{i}\" for i in range(mat.shape[1])])\n",
    "        df[article_id_col] = ids\n",
    "        return df\n",
    "\n",
    "    # Cas 2: pickle = matrice numpy\n",
    "    mat = np.array(emb_obj)\n",
    "    if mat.ndim != 2:\n",
    "        raise ValueError(\"Pickle embeddings non reconnu: attendu matrice 2D ou dict\")\n",
    "\n",
    "    # Hypothèse simple (fréquente sur ce dataset): embeddings alignés sur articles_metadata\n",
    "    ids = articles_meta[article_id_col].astype(int).to_numpy()\n",
    "\n",
    "    if len(ids) != mat.shape[0]:\n",
    "        raise ValueError(\n",
    "            f\"Mismatch: metadata a {len(ids)} ids mais embeddings a {mat.shape[0]} lignes. \"\n",
    "            \"Il faut retrouver l'ordre exact / la liste d'ids.\"\n",
    "        )\n",
    "\n",
    "    df = pd.DataFrame(mat, columns=[f\"e{i}\" for i in range(mat.shape[1])])\n",
    "    df[article_id_col] = ids\n",
    "    return df\n",
    "\n",
    "\n",
    "# Construction du DataFrame embeddings\n",
    "emb_df = build_embeddings_df(emb_obj, articles_meta, ARTICLE_ID_COL)\n",
    "print(\"emb_df shape:\", emb_df.shape)\n",
    "display(emb_df.head(3))\n",
    "\n",
    "# On indexe par article_id pour accès rapide\n",
    "# On enlève d'éventuels doublons d'ids\n",
    "emb_df = emb_df.drop_duplicates(subset=[ARTICLE_ID_COL]).set_index(ARTICLE_ID_COL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1211742-ff0f-4954-9e5c-dc47c031793f",
   "metadata": {},
   "source": [
    "## LightFM — Construire les mappings et les matrices (interactions + item_features)\n",
    "\n",
    "Objectifs :\n",
    "- créer un index dense pour les utilisateurs et articles (LightFM travaille sur 0..n-1),\n",
    "- construire la matrice `interactions` à partir des clics (implicite),\n",
    "- construire la matrice `item_features` à partir des embeddings (250D),\n",
    "- restreindre aux articles présents dans les embeddings (sinon pas de features).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4505b0b-22a7-49a8-b57d-2c0786af3ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bloc 1: préparation d'un train user/article minimal pour LightFM\n",
    "# On prend uniquement (user_id, article_id) et on filtre les articles sans embeddings.\n",
    "train_df = clicks_train_p[[COL_USER, COL_ITEM, \"ts\"]].copy()\n",
    "train_df = train_df.rename(columns={COL_USER: \"user_id\", COL_ITEM: \"article_id\"})\n",
    "\n",
    "train_df[\"user_id\"] = train_df[\"user_id\"].astype(int)\n",
    "train_df[\"article_id\"] = train_df[\"article_id\"].astype(int)\n",
    "\n",
    "# Filtrage: on garde uniquement les articles qui ont un embedding\n",
    "train_df = train_df[train_df[\"article_id\"].isin(emb_df.index)]\n",
    "print(\"Train clicks after embedding filter:\", train_df.shape)\n",
    "\n",
    "# --- Bloc 2: création des mappings (id réel -> index)\n",
    "# LightFM attend des indices 0..n-1.\n",
    "user_ids = np.sort(train_df[\"user_id\"].unique())\n",
    "item_ids = np.sort(train_df[\"article_id\"].unique())\n",
    "\n",
    "user_to_idx = {uid: i for i, uid in enumerate(user_ids)}\n",
    "item_to_idx = {aid: i for i, aid in enumerate(item_ids)}\n",
    "idx_to_item = {i: aid for aid, i in item_to_idx.items()}\n",
    "\n",
    "n_users = len(user_to_idx)\n",
    "n_items = len(item_to_idx)\n",
    "print(\"n_users:\", n_users, \"| n_items:\", n_items)\n",
    "\n",
    "# --- Bloc 3: construction de la matrice interactions (implicite)\n",
    "# Ici: interaction = 1 si (user,item) a été cliqué dans le train.\n",
    "ui = train_df.drop_duplicates(subset=[\"user_id\", \"article_id\"])\n",
    "\n",
    "u = ui[\"user_id\"].map(user_to_idx).to_numpy()\n",
    "i = ui[\"article_id\"].map(item_to_idx).to_numpy()\n",
    "data = np.ones(len(ui), dtype=np.float32)\n",
    "\n",
    "interactions = csr_matrix((data, (u, i)), shape=(n_users, n_items))\n",
    "print(\"interactions shape:\", interactions.shape, \"| nnz:\", interactions.nnz)\n",
    "\n",
    "# --- Bloc 4: construction de la matrice item_features (embeddings)\n",
    "# On construit une matrice (n_items x 250) alignée sur item_to_idx.\n",
    "emb_cols = [c for c in emb_df.columns if c.startswith(\"e\")]\n",
    "\n",
    "item_emb_matrix = np.zeros((n_items, len(emb_cols)), dtype=np.float32)\n",
    "for aid, idx in item_to_idx.items():\n",
    "    item_emb_matrix[idx] = emb_df.loc[aid, emb_cols].to_numpy(dtype=np.float32)\n",
    "\n",
    "# Conversion en sparse (LightFM accepte sparse)\n",
    "item_features = csr_matrix(item_emb_matrix)\n",
    "print(\"item_features shape:\", item_features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f53328-ee5a-43f2-8403-e0807f81849d",
   "metadata": {},
   "source": [
    "## LightFM — Entraînement (WARP) + évaluation MVP (par utilisateur)\n",
    "\n",
    "Objectifs :\n",
    "- entraîner un modèle LightFM en implicite avec embeddings (hybride),\n",
    "- évaluer avec un protocole MVP simple:\n",
    "  - pour chaque user, on prend son **premier clic en test** comme \"cible\",\n",
    "  - on recommande top-5 à partir de son historique train,\n",
    "  - on calcule HR@5 et MRR@5,\n",
    "  - fallback Trending si user inconnu (cold start).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb944ab4-a9d2-4170-a182-44c006341bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bloc 1: entraînement LightFM\n",
    "# WARP est adapté au ranking implicite (apprendre à mettre les bons items en haut).\n",
    "model = LightFM(loss=\"warp\", no_components=32, learning_rate=0.05, random_state=SEED)\n",
    "model.fit(interactions, item_features=item_features, epochs=10, num_threads=4)\n",
    "\n",
    "# --- Bloc 2: préparation du test \"par user\"\n",
    "# On prend le premier clic en test de chaque user comme \"vrai prochain item\".\n",
    "test_df = clicks_test_p[[COL_USER, COL_ITEM, \"ts\"]].copy()\n",
    "test_df = test_df.rename(columns={COL_USER: \"user_id\", COL_ITEM: \"article_id\"})\n",
    "\n",
    "test_df[\"user_id\"] = test_df[\"user_id\"].astype(int)\n",
    "test_df[\"article_id\"] = test_df[\"article_id\"].astype(int)\n",
    "test_df = test_df.sort_values([\"user_id\", \"ts\"], ascending=[True, True])\n",
    "\n",
    "first_test_click = test_df.groupby(\"user_id\", as_index=False).first()[[\"user_id\", \"article_id\"]]\n",
    "print(\"Users with at least 1 click in test:\", first_test_click.shape[0])\n",
    "\n",
    "# --- Fonction métriques: HR@k et MRR@k\n",
    "# HR@k: 1 si le vrai item est dans le top-k, sinon 0\n",
    "# MRR@k: 1/rang si présent, sinon 0\n",
    "def hr_mrr_at_k(true_item: int, recs: list[int], k: int = TOP_K) -> tuple[float, float]:\n",
    "    topk = recs[:k]\n",
    "    if true_item in topk:\n",
    "        rank = topk.index(true_item) + 1\n",
    "        return 1.0, 1.0 / rank\n",
    "    return 0.0, 0.0\n",
    "\n",
    "# --- Pré-calcul: tous les indices items, utile pour scorer rapidement\n",
    "all_item_idx = np.arange(n_items, dtype=np.int32)\n",
    "\n",
    "# --- Pré-calcul: historique train par user pour filtrer les items déjà vus\n",
    "user_seen = (\n",
    "    train_df.drop_duplicates(subset=[\"user_id\", \"article_id\"])\n",
    "    .groupby(\"user_id\")[\"article_id\"]\n",
    "    .apply(list)\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# --- Fonction recommandation LightFM\n",
    "# - si user inconnu -> fallback trending\n",
    "# - sinon, on score tous les items et on prend les meilleurs\n",
    "# - on filtre les items déjà vus\n",
    "# - on complète avec trending si nécessaire\n",
    "def recommend_lightfm(user_id: int, k: int = TOP_K) -> list[int]:\n",
    "    \"\"\"\n",
    "    Recommande top-k articles pour un user:\n",
    "    - scores LightFM (hybride embeddings)\n",
    "    - filtrage des items déjà vus\n",
    "    - fallback trending si besoin\n",
    "    \"\"\"\n",
    "    if user_id not in user_to_idx:\n",
    "        return recommend_trending(k=k)\n",
    "\n",
    "    uidx = user_to_idx[user_id]\n",
    "    scores = model.predict(uidx, all_item_idx, item_features=item_features)\n",
    "\n",
    "    # On prend plus que k pour pouvoir filtrer les \"seen\"\n",
    "    candidate_n = k * 20\n",
    "    top_idx = np.argpartition(-scores, candidate_n)[:candidate_n]\n",
    "    top_idx = top_idx[np.argsort(-scores[top_idx])]\n",
    "\n",
    "    seen = set(user_seen.get(user_id, []))\n",
    "    recs = []\n",
    "    for ii in top_idx:\n",
    "        aid = idx_to_item[int(ii)]\n",
    "        if aid in seen:\n",
    "            continue\n",
    "        recs.append(aid)\n",
    "        if len(recs) >= k:\n",
    "            break\n",
    "\n",
    "    # Complète si on n'a pas assez de recos\n",
    "    if len(recs) < k:\n",
    "        for aid in TRENDING_LIST:\n",
    "            if aid not in seen and aid not in recs:\n",
    "                recs.append(aid)\n",
    "            if len(recs) >= k:\n",
    "                break\n",
    "\n",
    "    return recs\n",
    "\n",
    "# --- Bloc 3: boucle d'évaluation\n",
    "# On compare les recommandations au vrai item test de chaque user.\n",
    "rows = []\n",
    "for _, r in first_test_click.iterrows():\n",
    "    uid = int(r[\"user_id\"])\n",
    "    true_item = int(r[\"article_id\"])\n",
    "\n",
    "    recs = recommend_lightfm(uid, k=TOP_K)\n",
    "    hr, mrr = hr_mrr_at_k(true_item, recs, k=TOP_K)\n",
    "    rows.append({\"user_id\": uid, \"true_item\": true_item, \"HR@5\": hr, \"MRR@5\": mrr})\n",
    "\n",
    "eval_lightfm = pd.DataFrame(rows)\n",
    "\n",
    "# --- Bloc 4: résumé des scores\n",
    "summary_lightfm = pd.DataFrame(\n",
    "    {\n",
    "        \"model\": [\"LightFM (hybride embeddings)\"],\n",
    "        \"n_users_eval\": [len(eval_lightfm)],\n",
    "        \"HR@5\": [eval_lightfm[\"HR@5\"].mean()],\n",
    "        \"MRR@5\": [eval_lightfm[\"MRR@5\"].mean()],\n",
    "    }\n",
    ")\n",
    "\n",
    "display(summary_lightfm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5c33dd-ddb9-4c38-a271-7f28468dade9",
   "metadata": {},
   "source": [
    "## LightFM — Pré-calculer les recommandations top-5 par user (pour déploiement simple)\n",
    "\n",
    "Objectif MVP :\n",
    "- calculer offline (dans le notebook) les **5 recommandations** pour chaque utilisateur connu du modèle,\n",
    "- sauvegarder un fichier léger `user_top5.parquet`,\n",
    "- en production (Azure Function) : faire un **lookup** par `user_id` + fallback Trending.\n",
    "\n",
    "Remarque :\n",
    "- on filtre les articles déjà vus dans le train,\n",
    "- on complète avec Trending si on n’a pas assez de candidats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43208a5-8650-4e5e-9e68-fac4e5db9daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bloc 1: fonction de génération top-k pour un user index (LightFM)\n",
    "# On réutilise model, item_features, all_item_idx, idx_to_item, user_seen, TRENDING_LIST.\n",
    "\n",
    "def topk_for_user(uid: int, k: int = TOP_K, candidate_mult: int = 50) -> list[int]:\n",
    "    \"\"\"\n",
    "    Génère top-k recommandations LightFM pour un user connu (uid réel).\n",
    "    - candidate_mult: on prend k*candidate_mult candidats pour compenser le filtrage \"seen\".\n",
    "    \"\"\"\n",
    "    if uid not in user_to_idx:\n",
    "        return recommend_trending(k=k)\n",
    "\n",
    "    uidx = user_to_idx[uid]\n",
    "    scores = model.predict(uidx, all_item_idx, item_features=item_features)\n",
    "\n",
    "    candidate_n = min(len(scores), k * candidate_mult)\n",
    "    top_idx = np.argpartition(-scores, candidate_n)[:candidate_n]\n",
    "    top_idx = top_idx[np.argsort(-scores[top_idx])]\n",
    "\n",
    "    seen = set(user_seen.get(uid, []))\n",
    "    recs = []\n",
    "\n",
    "    for ii in top_idx:\n",
    "        aid = idx_to_item[int(ii)]\n",
    "        if aid in seen:\n",
    "            continue\n",
    "        recs.append(aid)\n",
    "        if len(recs) >= k:\n",
    "            break\n",
    "\n",
    "    # Complète avec trending si besoin\n",
    "    if len(recs) < k:\n",
    "        for aid in TRENDING_LIST:\n",
    "            if aid not in seen and aid not in recs:\n",
    "                recs.append(aid)\n",
    "            if len(recs) >= k:\n",
    "                break\n",
    "\n",
    "    return recs\n",
    "\n",
    "\n",
    "# --- Bloc 2: calcul des top-5 pour tous les users du modèle\n",
    "rows = []\n",
    "for uid in user_to_idx.keys():\n",
    "    rows.append({\"user_id\": int(uid), \"recommended_articles\": topk_for_user(int(uid), k=TOP_K)})\n",
    "\n",
    "user_top5 = pd.DataFrame(rows)\n",
    "display(user_top5.head())\n",
    "\n",
    "print(\"Nb users:\", len(user_top5))\n",
    "print(\"Exemple recos:\", user_top5.iloc[0][\"recommended_articles\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3defa6-6e04-497a-a6da-326b740d47b1",
   "metadata": {},
   "source": [
    "## Export artefacts MVP (fichiers à mettre dans Blob)\n",
    "\n",
    "Objectif :\n",
    "- sauvegarder les artefacts minimaux nécessaires au serving “lookup-only”.\n",
    "- fichiers recommandés :\n",
    "  - `user_top5.parquet` : recommandations pré-calculées\n",
    "  - `trending.parquet` : fallback cold start\n",
    "  - (optionnel) `meta.json` : version/infos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cc972e-8268-44d8-9704-adaed530b713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "ARTIFACTS_DIR = \"artifacts_lightfm\"\n",
    "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
    "\n",
    "# Fichier principal: lookup top-5\n",
    "user_top5_path = os.path.join(ARTIFACTS_DIR, \"user_top5.parquet\")\n",
    "user_top5.to_parquet(user_top5_path, index=False)\n",
    "\n",
    "# Fallback trending\n",
    "trending_path = os.path.join(ARTIFACTS_DIR, \"trending.parquet\")\n",
    "trending.to_parquet(trending_path, index=False)\n",
    "\n",
    "# Petit méta (optionnel)\n",
    "meta = {\n",
    "    \"model\": \"LightFM_hybrid_embeddings\",\n",
    "    \"top_k\": TOP_K,\n",
    "    \"n_users\": int(len(user_top5)),\n",
    "    \"n_items\": int(n_items),\n",
    "    \"loss\": \"warp\",\n",
    "    \"no_components\": 32,\n",
    "    \"learning_rate\": 0.05,\n",
    "}\n",
    "with open(os.path.join(ARTIFACTS_DIR, \"meta.json\"), \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(\"Artefacts écrits dans:\", ARTIFACTS_DIR)\n",
    "print(\" -\", user_top5_path)\n",
    "print(\" -\", trending_path)\n",
    "print(\" -\", os.path.join(ARTIFACTS_DIR, \"meta.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e78bba3-d29a-4440-a3b0-d93a123d96c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de logique \"serving\" en local (identique côté Azure Function)\n",
    "\n",
    "user_top5_loaded = pd.read_parquet(user_top5_path)\n",
    "trending_loaded = pd.read_parquet(trending_path)\n",
    "\n",
    "top5_map = dict(zip(user_top5_loaded[\"user_id\"].astype(int), user_top5_loaded[\"recommended_articles\"]))\n",
    "trending_list = trending_loaded[\"article_id\"].astype(int).tolist()\n",
    "\n",
    "def recommend_serving_lookup(user_id: int, k: int = TOP_K) -> tuple[list[int], str]:\n",
    "    recos = top5_map.get(int(user_id))\n",
    "    if recos is None:\n",
    "        return trending_list[:k], \"trending\"\n",
    "    return list(recos)[:k], \"lightfm_precomputed\"\n",
    "\n",
    "# Test rapide\n",
    "print(recommend_serving_lookup(user_id=list(top5_map.keys())[0]))\n",
    "print(recommend_serving_lookup(user_id=-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892745ed-27b4-47c7-98d6-8b7066d22c73",
   "metadata": {},
   "source": [
    "## Scénario 2 — Serving en ligne (Azure Function) : charger le modèle et prédire à la demande\n",
    "\n",
    "Objectif :\n",
    "- **ne pas** pré-calculer par user,\n",
    "- **charger** les artefacts (modèle + matrices + mappings) au démarrage de la Function,\n",
    "- **inférer en ligne** : `GET /recommend?user_id=...` → renvoie 5 articles.\n",
    "\n",
    "Principe :\n",
    "- au *cold start*, on charge et on met en cache :\n",
    "  - le modèle LightFM,\n",
    "  - `item_features` (embeddings en sparse),\n",
    "  - les mappings (`user_to_idx`, `idx_to_item`, `user_seen`),\n",
    "  - `trending` (fallback).\n",
    "- à chaque requête :\n",
    "  - user connu → score + top-5\n",
    "  - user inconnu → trending\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92689f50-2cb5-41cc-a4f4-02f7ba3b1378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export des artefacts pour le serving en ligne\n",
    "# On sauvegarde tout ce qui est nécessaire pour faire model.predict(...) côté Azure Function.\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from scipy.sparse import save_npz\n",
    "\n",
    "ARTIFACTS_DIR = \"artifacts_lightfm_online\"\n",
    "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
    "\n",
    "# 1) Sauvegarde du modèle LightFM\n",
    "model_path = os.path.join(ARTIFACTS_DIR, \"lightfm_model.pkl\")\n",
    "with open(model_path, \"wb\") as f:\n",
    "    pickle.dump(model, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# 2) Sauvegarde des item_features (embeddings) au format sparse .npz\n",
    "item_features_path = os.path.join(ARTIFACTS_DIR, \"item_features.npz\")\n",
    "save_npz(item_features_path, item_features)\n",
    "\n",
    "# 3) Sauvegarde des mappings indispensables au serving\n",
    "# - user_to_idx: user_id réel -> index LightFM\n",
    "# - idx_to_item: index LightFM -> article_id réel\n",
    "# - user_seen: articles vus par user (pour filtrer les recos déjà vues)\n",
    "mappings = {\n",
    "    \"user_to_idx\": user_to_idx,\n",
    "    \"idx_to_item\": idx_to_item,\n",
    "    \"user_seen\": user_seen,\n",
    "    \"top_k\": TOP_K,\n",
    "}\n",
    "mappings_path = os.path.join(ARTIFACTS_DIR, \"mappings.pkl\")\n",
    "with open(mappings_path, \"wb\") as f:\n",
    "    pickle.dump(mappings, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# 4) Trending (fallback cold start user)\n",
    "trending_path = os.path.join(ARTIFACTS_DIR, \"trending.parquet\")\n",
    "trending.to_parquet(trending_path, index=False)\n",
    "\n",
    "# 5) Meta (optionnel mais utile pour debug/traçabilité)\n",
    "meta = {\n",
    "    \"model\": \"LightFM_online\",\n",
    "    \"loss\": \"warp\",\n",
    "    \"no_components\": 32,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"n_users\": int(len(user_to_idx)),\n",
    "    \"n_items\": int(n_items),\n",
    "    \"top_k\": int(TOP_K),\n",
    "}\n",
    "meta_path = os.path.join(ARTIFACTS_DIR, \"meta.json\")\n",
    "with open(meta_path, \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(\"Artefacts exportés dans:\", ARTIFACTS_DIR)\n",
    "print(\" -\", model_path)\n",
    "print(\" -\", item_features_path)\n",
    "print(\" -\", mappings_path)\n",
    "print(\" -\", trending_path)\n",
    "print(\" -\", meta_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a655561-16c8-4ff2-bfa6-13435a1c9ea8",
   "metadata": {},
   "source": [
    "## Test local “comme en prod” (charger les artefacts et prédire)\n",
    "\n",
    "Objectif :\n",
    "- simuler exactement ce que fera l’Azure Function :\n",
    "  - charger modèle + matrices + mappings,\n",
    "  - répondre à un `user_id` en renvoyant 5 articles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d84c60-ad4f-4269-955a-ce98c5951dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test local du serving en ligne (chargement depuis disque)\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "# --- Chargement artefacts\n",
    "with open(model_path, \"rb\") as f:\n",
    "    model_loaded = pickle.load(f)\n",
    "\n",
    "item_features_loaded = load_npz(item_features_path)\n",
    "\n",
    "with open(mappings_path, \"rb\") as f:\n",
    "    m_loaded = pickle.load(f)\n",
    "\n",
    "user_to_idx_loaded = m_loaded[\"user_to_idx\"]\n",
    "idx_to_item_loaded = m_loaded[\"idx_to_item\"]\n",
    "user_seen_loaded = m_loaded[\"user_seen\"]\n",
    "TOP_K_LOADED = m_loaded[\"top_k\"]\n",
    "\n",
    "trending_loaded = pd.read_parquet(trending_path)\n",
    "TRENDING_LIST_LOADED = trending_loaded[\"article_id\"].astype(int).tolist()\n",
    "\n",
    "# --- Pré-calcul utile\n",
    "n_items_loaded = item_features_loaded.shape[0]\n",
    "all_item_idx_loaded = np.arange(n_items_loaded, dtype=np.int32)\n",
    "\n",
    "# --- Fonction: recommander en ligne\n",
    "def recommend_online(user_id: int, k: int = TOP_K_LOADED) -> tuple[list[int], str]:\n",
    "    \"\"\"\n",
    "    Reco online:\n",
    "    - user inconnu -> trending\n",
    "    - user connu -> scores LightFM + filtrage 'seen' + fallback trending\n",
    "    \"\"\"\n",
    "    if user_id not in user_to_idx_loaded:\n",
    "        return TRENDING_LIST_LOADED[:k], \"trending\"\n",
    "\n",
    "    uidx = user_to_idx_loaded[user_id]\n",
    "    scores = model_loaded.predict(uidx, all_item_idx_loaded, item_features=item_features_loaded)\n",
    "\n",
    "    # On prend plus de candidats que k pour pouvoir filtrer les items déjà vus\n",
    "    candidate_n = min(len(scores), k * 50)\n",
    "    top_idx = np.argpartition(-scores, candidate_n)[:candidate_n]\n",
    "    top_idx = top_idx[np.argsort(-scores[top_idx])]\n",
    "\n",
    "    seen = set(user_seen_loaded.get(user_id, []))\n",
    "    recs = []\n",
    "\n",
    "    for ii in top_idx:\n",
    "        aid = idx_to_item_loaded[int(ii)]\n",
    "        if aid in seen:\n",
    "            continue\n",
    "        recs.append(int(aid))\n",
    "        if len(recs) >= k:\n",
    "            break\n",
    "\n",
    "    # Complète avec trending si besoin\n",
    "    if len(recs) < k:\n",
    "        for aid in TRENDING_LIST_LOADED:\n",
    "            if aid not in seen and aid not in recs:\n",
    "                recs.append(int(aid))\n",
    "            if len(recs) >= k:\n",
    "                break\n",
    "\n",
    "    return recs, \"lightfm_online\"\n",
    "\n",
    "# Test rapide\n",
    "some_user = next(iter(user_to_idx_loaded.keys()))\n",
    "print(\"User connu:\", some_user, recommend_online(some_user))\n",
    "print(\"User inconnu:\", -1, recommend_online(-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22264386-5ca2-477c-a6d5-9d95f350b4cf",
   "metadata": {},
   "source": [
    "## Azure Function minimale (HTTP) — fichiers à créer\n",
    "\n",
    "Objectif :\n",
    "- exposer un endpoint :\n",
    "  - `GET /api/recommend?user_id=123`\n",
    "- charger les artefacts une seule fois (cache global),\n",
    "- renvoyer un JSON : `{ user_id, recommended_articles, strategy }`.\n",
    "\n",
    "Hypothèse MVP :\n",
    "- les artefacts sont présents sur le filesystem de la Function (déploiement simple),\n",
    "  ou montés / téléchargés depuis Blob (à ajouter ensuite).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb07842-9ed2-466f-b329-9d028dd4a2d8",
   "metadata": {},
   "source": [
    "## Notes déploiement (MVP)\n",
    "\n",
    "- Déposer le dossier `artifacts_lightfm_online/` à côté de la Function (ou le copier dans le package).\n",
    "- Définir la variable d’environnement `ARTIFACTS_DIR` si besoin.\n",
    "- Exemple d’appel :\n",
    "  - `GET https://<ton-host>/api/recommend?user_id=123`\n",
    "  - (optionnel) `&k=5`\n",
    "\n",
    "Dépendances (requirements.txt) à prévoir pour Azure Functions :\n",
    "- `azure-functions`\n",
    "- `pandas`\n",
    "- `numpy`\n",
    "- `scipy`\n",
    "- `lightfm`\n",
    "- `pyarrow` (si tu lis/écris du parquet)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab874028-0b33-414e-901e-61f2e9863ad4",
   "metadata": {},
   "source": [
    "## Mesurer les temps (MVP) : chargement des artefacts + inférence\n",
    "\n",
    "Objectif :\n",
    "- estimer le coût du *cold start* (temps pour charger modèle + matrices + mappings),\n",
    "- estimer le coût d’une requête *warm* (temps pour scorer + extraire top-5),\n",
    "- avoir une idée du risque de latence en Azure Functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e56670-8f13-4507-b879-a58a869d0348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "# --- Bloc 1: mesure du temps de chargement (simule un cold start)\n",
    "t0 = time.perf_counter()\n",
    "\n",
    "with open(model_path, \"rb\") as f:\n",
    "    model_loaded = pickle.load(f)\n",
    "\n",
    "item_features_loaded = load_npz(item_features_path)\n",
    "\n",
    "with open(mappings_path, \"rb\") as f:\n",
    "    m_loaded = pickle.load(f)\n",
    "\n",
    "user_to_idx_loaded = m_loaded[\"user_to_idx\"]\n",
    "idx_to_item_loaded = m_loaded[\"idx_to_item\"]\n",
    "user_seen_loaded = m_loaded[\"user_seen\"]\n",
    "TOP_K_LOADED = int(m_loaded.get(\"top_k\", TOP_K))\n",
    "\n",
    "trending_loaded = pd.read_parquet(trending_path)\n",
    "TRENDING_LIST_LOADED = trending_loaded[\"article_id\"].astype(int).tolist()\n",
    "\n",
    "n_items_loaded = item_features_loaded.shape[0]\n",
    "all_item_idx_loaded = np.arange(n_items_loaded, dtype=np.int32)\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "print(f\"Temps de chargement artefacts (cold start simulé): {(t1 - t0):.3f} s\")\n",
    "\n",
    "\n",
    "# --- Bloc 2: fonction de recommandation (identique au serving)\n",
    "def recommend_online(user_id: int, k: int = TOP_K_LOADED) -> tuple[list[int], str]:\n",
    "    # Cold start user\n",
    "    if user_id not in user_to_idx_loaded:\n",
    "        return TRENDING_LIST_LOADED[:k], \"trending\"\n",
    "\n",
    "    uidx = user_to_idx_loaded[user_id]\n",
    "    scores = model_loaded.predict(uidx, all_item_idx_loaded, item_features=item_features_loaded)\n",
    "\n",
    "    # Sur-échantillonne pour filtrer les items déjà vus\n",
    "    candidate_n = min(len(scores), k * 50)\n",
    "    top_idx = np.argpartition(-scores, candidate_n)[:candidate_n]\n",
    "    top_idx = top_idx[np.argsort(-scores[top_idx])]\n",
    "\n",
    "    seen = set(user_seen_loaded.get(user_id, []))\n",
    "    recs = []\n",
    "\n",
    "    for ii in top_idx:\n",
    "        aid = idx_to_item_loaded[int(ii)]\n",
    "        if aid in seen:\n",
    "            continue\n",
    "        recs.append(int(aid))\n",
    "        if len(recs) >= k:\n",
    "            break\n",
    "\n",
    "    # Complète si besoin\n",
    "    if len(recs) < k:\n",
    "        for aid in TRENDING_LIST_LOADED:\n",
    "            if aid not in seen and aid not in recs:\n",
    "                recs.append(int(aid))\n",
    "            if len(recs) >= k:\n",
    "                break\n",
    "\n",
    "    return recs, \"lightfm_online\"\n",
    "\n",
    "\n",
    "# --- Bloc 3: mesure du temps d'inférence (warm)\n",
    "# On teste sur 1 user connu (si possible) et sur un user inconnu.\n",
    "known_user = next(iter(user_to_idx_loaded.keys()))\n",
    "unknown_user = -1\n",
    "\n",
    "def time_inference(user_id: int, n_runs: int = 20):\n",
    "    # Petit warm-up (premier appel peut être plus lent)\n",
    "    recommend_online(user_id)\n",
    "\n",
    "    times = []\n",
    "    for _ in range(n_runs):\n",
    "        t0 = time.perf_counter()\n",
    "        _ = recommend_online(user_id)\n",
    "        t1 = time.perf_counter()\n",
    "        times.append(t1 - t0)\n",
    "\n",
    "    times = np.array(times)\n",
    "    print(f\"user_id={user_id} | mean={times.mean()*1000:.2f} ms | p95={np.percentile(times,95)*1000:.2f} ms | max={times.max()*1000:.2f} ms\")\n",
    "\n",
    "time_inference(known_user, n_runs=30)\n",
    "time_inference(unknown_user, n_runs=30)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
